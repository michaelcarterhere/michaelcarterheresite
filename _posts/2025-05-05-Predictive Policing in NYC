---
layout: post
title: "Predictive Policing in NYC"
Date: 2025-05-05
---
Crime Prediction

Smart Cities 2.0: Is the NYC MTA going full Minority Report?

Last week, Michael Kemper, Chief Security Officer for the MTA gave a presentation at the Safety Committee Meeting. He shared that the NYC MTA is testing artificial intelligence to detect “problematic behavior” in subway stations, using real-time video feeds to flag incidents before they occur. It’s not facial recognition, but it is predictive surveillance—designed to notify human operators when something seems off.

A few important questions come to mind:
	•	Who decides what behavior counts as “suspicious”?
	•	How are these systems trained—and how do we detect or correct bias?
	•	What kind of governance and public accountability should surround AI used in shared spaces?
	•	And what does “smart” mean when it comes to cities—efficient, safe, surveilled?


This MTA pilot isn’t just about transit. It’s about the politics of automation in the places we move through, work in, and rely on every day.

AI is shaping the future of cities—and how we define freedom, risk, and trust in them.

If we want equitable, human-centered smart cities, we need to talk not just about what’s possible, but what’s permissible.

What’s your take on AI-powered surveillance in public space?
![alt text](../assets/images/img-2.jpeg)